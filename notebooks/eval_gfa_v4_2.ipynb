{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP4CAD-GFA v4.2 Evaluation\n",
    "\n",
    "## Metrics\n",
    "1. **Self-grounding quality**: cosine(z_guided, z_self) - target > 0.85\n",
    "2. **Query alignment**: cosine(T_feat, Q_self) - target > 0.7\n",
    "3. **Retrieval**: Text→BRep, Text→PC R@1, R@5, R@10\n",
    "4. **Self-path gap**: guided R@1 - self R@1 - target < 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Imports and Setup\nimport sys\nsys.path.insert(0, '..')\n\nimport os\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom pathlib import Path\n\nfrom clip4cad.models import CLIP4CAD_GFA_v4_2, GFAv4_2Config\nfrom clip4cad.losses.gfa_v4_2_losses import compute_self_grounding_quality, compute_query_alignment\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Data Paths\n\nDATA_ROOT = Path(\"d:/Defect_Det/MMCAD/data\")\nPC_FILE = Path(\"c:/Users/User/Desktop/pc_embeddings_full.h5\")\nBREP_FILE = Path(\"c:/Users/User/Desktop/brep_features.h5\")\nTEXT_FILE = Path(\"c:/Users/User/Desktop/text_embeddings.h5\")\nMODEL_PATH = Path(\"../outputs/gfa_v4_2/clip4cad_gfa_v4_2_final.pt\")\n\nprint(f\"Data root: {DATA_ROOT}\")\nprint(f\"Model: {MODEL_PATH} (exists: {MODEL_PATH.exists()})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Load Model\n\ncheckpoint = torch.load(MODEL_PATH, map_location=device)\nconfig = GFAv4_2Config(**checkpoint['config'])\n\nmodel = CLIP4CAD_GFA_v4_2(config).to(device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\n# Set to fully independent mode (no hints)\nmodel.set_cond_dropout(1.0)\n\nprint(f\"Loaded model with {model.count_parameters():,} parameters\")\nprint(f\"Conditioning dropout: 1.0 (fully independent)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load Validation Data\n\nfrom clip4cad.data.gfa_dataset import GFAMappedDataset, gfa_collate_fn\n\nprint(\"Loading validation data...\")\nval_dataset = GFAMappedDataset(\n    data_root=str(DATA_ROOT),\n    split=\"val\",\n    pc_file=str(PC_FILE),\n    text_file=str(TEXT_FILE),\n    brep_file=str(BREP_FILE),\n    num_rotations=1,\n    load_to_memory=False,\n    use_live_text=False,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=256,\n    shuffle=False,\n    num_workers=0,\n    collate_fn=gfa_collate_fn,\n    pin_memory=True\n)\n\nprint(f\"Validation samples: {len(val_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Collect Embeddings\n\nall_z_brep_guided = []\nall_z_pc_guided = []\nall_z_brep_self = []\nall_z_pc_self = []\nall_z_text = []\nall_T_feat = []\nall_Q_brep_self = []\nall_Q_pc_self = []\nall_confidence = []\nall_uids = []\n\nprint(\"Extracting embeddings...\")\nwith torch.no_grad():\n    for batch in tqdm(val_loader):\n        outputs = model(batch)\n        \n        all_z_brep_guided.append(outputs['z_brep'].cpu())\n        all_z_pc_guided.append(outputs['z_pc'].cpu())\n        all_z_brep_self.append(outputs['z_brep_self'].cpu())\n        all_z_pc_self.append(outputs['z_pc_self'].cpu())\n        all_z_text.append(outputs['z_text'].cpu())\n        all_T_feat.append(outputs['T_feat'].cpu())\n        all_Q_brep_self.append(outputs['Q_brep_self'].cpu())\n        all_Q_pc_self.append(outputs['Q_pc_self'].cpu())\n        all_confidence.append(outputs['confidence'].cpu())\n        all_uids.extend(batch.get('sample_id', [f\"sample_{i}\" for i in range(len(outputs['z_brep']))]))\n\nz_brep_guided = torch.cat(all_z_brep_guided, dim=0)\nz_pc_guided = torch.cat(all_z_pc_guided, dim=0)\nz_brep_self = torch.cat(all_z_brep_self, dim=0)\nz_pc_self = torch.cat(all_z_pc_self, dim=0)\nz_text = torch.cat(all_z_text, dim=0)\nT_feat = torch.cat(all_T_feat, dim=0)\nQ_brep_self = torch.cat(all_Q_brep_self, dim=0)\nQ_pc_self = torch.cat(all_Q_pc_self, dim=0)\nconfidence = torch.cat(all_confidence, dim=0)\n\nprint(f\"\\nEmbeddings collected: {len(all_uids)} samples\")\nprint(f\"  z_brep_guided: {z_brep_guided.shape}\")\nprint(f\"  z_brep_self: {z_brep_self.shape}\")\nprint(f\"  T_feat: {T_feat.shape}\")\nprint(f\"  Q_brep_self: {Q_brep_self.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Self-grounding quality\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SELF-GROUNDING QUALITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "self_cos_brep = compute_self_grounding_quality(z_brep_guided, z_brep_self)\n",
    "self_cos_pc = compute_self_grounding_quality(z_pc_guided, z_pc_self)\n",
    "\n",
    "print(f\"BRep: {self_cos_brep:.4f} (target > 0.85)\")\n",
    "print(f\"PC:   {self_cos_pc:.4f} (target > 0.85)\")\n",
    "print(f\"Avg:  {(self_cos_brep + self_cos_pc) / 2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Query alignment\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"QUERY ALIGNMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "q_align_brep = compute_query_alignment(T_feat, Q_brep_self, confidence)\n",
    "q_align_pc = compute_query_alignment(T_feat, Q_pc_self, confidence)\n",
    "\n",
    "print(f\"BRep: {q_align_brep:.4f} (target > 0.7)\")\n",
    "print(f\"PC:   {q_align_pc:.4f} (target > 0.7)\")\n",
    "print(f\"Avg:  {(q_align_brep + q_align_pc) / 2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Retrieval metrics\n",
    "def compute_recall_at_k(queries, keys, k_values=[1, 5, 10]):\n",
    "    \"\"\"Compute Recall@K for retrieval.\"\"\"\n",
    "    queries = F.normalize(queries, dim=-1)\n",
    "    keys = F.normalize(keys, dim=-1)\n",
    "    \n",
    "    sim = queries @ keys.T  # (N, N)\n",
    "    \n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        _, topk_indices = sim.topk(k, dim=-1)\n",
    "        correct = (topk_indices == torch.arange(len(queries)).unsqueeze(1)).any(dim=1)\n",
    "        results[f'R@{k}'] = correct.float().mean().item() * 100\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RETRIEVAL METRICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Text → BRep (guided)\n",
    "recall_brep_guided = compute_recall_at_k(z_text, z_brep_guided)\n",
    "print(f\"\\nText → BRep (guided):\")\n",
    "for k, v in recall_brep_guided.items():\n",
    "    print(f\"  {k}: {v:.2f}%\")\n",
    "\n",
    "# Text → BRep (self)\n",
    "recall_brep_self = compute_recall_at_k(z_text, z_brep_self)\n",
    "print(f\"\\nText → BRep (self):\")\n",
    "for k, v in recall_brep_self.items():\n",
    "    print(f\"  {k}: {v:.2f}%\")\n",
    "\n",
    "# Text → PC (guided)\n",
    "recall_pc_guided = compute_recall_at_k(z_text, z_pc_guided)\n",
    "print(f\"\\nText → PC (guided):\")\n",
    "for k, v in recall_pc_guided.items():\n",
    "    print(f\"  {k}: {v:.2f}%\")\n",
    "\n",
    "# Text → PC (self)\n",
    "recall_pc_self = compute_recall_at_k(z_text, z_pc_self)\n",
    "print(f\"\\nText → PC (self):\")\n",
    "for k, v in recall_pc_self.items():\n",
    "    print(f\"  {k}: {v:.2f}%\")\n",
    "\n",
    "# Gap\n",
    "gap_brep = recall_brep_guided['R@1'] - recall_brep_self['R@1']\n",
    "gap_pc = recall_pc_guided['R@1'] - recall_pc_self['R@1']\n",
    "print(f\"\\nGap (guided - self):\")\n",
    "print(f\"  BRep: {gap_brep:.2f}% (target < 10%)\")\n",
    "print(f\"  PC:   {gap_pc:.2f}% (target < 10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. UMAP visualization\n",
    "print(\"\\nComputing UMAP embeddings...\")\n",
    "\n",
    "# Sample for visualization\n",
    "n_samples = min(2000, len(z_text))\n",
    "indices = np.random.choice(len(z_text), n_samples, replace=False)\n",
    "\n",
    "# Combine embeddings\n",
    "combined = torch.cat([\n",
    "    z_brep_guided[indices],\n",
    "    z_brep_self[indices],\n",
    "    z_text[indices]\n",
    "], dim=0).numpy()\n",
    "\n",
    "# UMAP\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "coords = tsne.fit_transform(combined)\n",
    "\n",
    "# Split back\n",
    "brep_guided_coords = coords[:n_samples]\n",
    "brep_self_coords = coords[n_samples:2*n_samples]\n",
    "text_coords = coords[2*n_samples:]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(brep_guided_coords[:, 0], brep_guided_coords[:, 1], c='green', alpha=0.5, s=10, label='BRep (guided)')\n",
    "plt.scatter(brep_self_coords[:, 0], brep_self_coords[:, 1], c='orange', alpha=0.5, s=10, label='BRep (self)')\n",
    "plt.scatter(text_coords[:, 0], text_coords[:, 1], c='blue', alpha=0.5, s=10, label='Text')\n",
    "plt.legend()\n",
    "plt.title('t-SNE: Guided vs Self Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSelf-grounding quality:\")\n",
    "print(f\"  BRep: {self_cos_brep:.4f}\")\n",
    "print(f\"  PC:   {self_cos_pc:.4f}\")\n",
    "print(f\"\\nQuery alignment:\")\n",
    "print(f\"  BRep: {q_align_brep:.4f}\")\n",
    "print(f\"  PC:   {q_align_pc:.4f}\")\n",
    "print(f\"\\nText→BRep R@1:\")\n",
    "print(f\"  Guided: {recall_brep_guided['R@1']:.2f}%\")\n",
    "print(f\"  Self:   {recall_brep_self['R@1']:.2f}%\")\n",
    "print(f\"  Gap:    {gap_brep:.2f}%\")\n",
    "print(f\"\\nText→PC R@1:\")\n",
    "print(f\"  Guided: {recall_pc_guided['R@1']:.2f}%\")\n",
    "print(f\"  Self:   {recall_pc_self['R@1']:.2f}%\")\n",
    "print(f\"  Gap:    {gap_pc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}