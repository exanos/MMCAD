# CLIP4CAD-GFA Model Configuration
# Cross-Modal Representation Learning via Grounded Feature Alignment

# ============================================================================
# Core Dimensions
# ============================================================================

# Unified representation dimension (all modalities projected to this)
d_unified: 256

# Contrastive projection dimension (for InfoNCE losses)
d_proj: 128

# Grounding space dimension (for computing grounding matrices)
d_ground: 128

# Alignment space dimension (for cross-modal consistency)
d_align: 128

# ============================================================================
# Grounding Architecture
# ============================================================================

# Number of feature slot queries (max distinct features to extract from text)
num_feature_slots: 12

# Attention configuration
num_attention_heads: 8  # Must divide d_unified (256 / 8 = 32)
num_parser_layers: 2

# Regularization
dropout: 0.1

# Sequence length limits
max_brep_tokens: 704    # Max faces (192) + edges (512)
max_text_tokens: 512    # Max description length

# Confidence threshold for active feature slots
confidence_threshold: 0.3

# ============================================================================
# Temperature Initialization
# ============================================================================

tau_contrastive_init: 0.07
tau_ground_init: 0.1

# ============================================================================
# Encoder Configurations (Pre-computed Features)
# ============================================================================

encoders:
  # B-Rep encoder (AutoBrep-style FSQ VAE)
  # Features are pre-computed using scripts/precompute_brep_features_step.py
  brep:
    # Output dimensions (matching AutoBrep XAEncoder)
    face_dim: 48  # 3 * 16 (surfZ dimension)
    edge_dim: 12  # 3 * 4 (edgeZ dimension)

    # Input sizes (for reference)
    face_grid_size: 32  # 32x32 UV grid
    edge_curve_size: 32  # 32 points per edge

    # Limits
    max_faces: 192
    max_edges: 512

    # Pretrained weights (auto-downloaded from HuggingFace if not specified)
    # Set to "auto" or null to auto-download from SamGiantEagle/AutoBrep
    surface_checkpoint: auto
    edge_checkpoint: auto
    auto_download: true  # Auto-download from HuggingFace if checkpoints not found

    # Pre-computed features are required for GFA training
    use_cached_features: true

  # Point cloud encoder (ShapeLLM/ReCon++ pre-computed)
  # Features are provided by user (finetuned on 32K CAD models)
  pointcloud:
    model: shapellm_precomputed  # Changed from ulip2-pointbert

    # ShapeLLM/ReCon++ architecture (for reference)
    # - 24-layer transformer, 16 heads, embed_dim=1024
    # - 512 groups pooled to 32 target_groups

    # Output dimensions (from ShapeLLM)
    output_dim: 1024        # Changed from 768
    num_tokens: 48          # 32 local_features + 16 global_token

    # Feature types in HDF5 (local_features is primary)
    use_global_token: true  # Concatenate global_token for 33 tokens

    # Pre-computed features are required for GFA training
    use_cached_features: true

  # Text encoder (LLM)
  # Features are pre-computed using scripts/precompute_text_embeddings.py
  text:
    model_name: microsoft/Phi-4-mini-instruct
    hidden_dim: 3072  # Phi-4-mini hidden dimension

    # Max sequence lengths
    max_title_len: 64
    max_desc_len: 256

    # Pre-computed embeddings are required for GFA training
    use_cached_embeddings: true

# ============================================================================
# Rotation Augmentation (DISABLED)
# ============================================================================

# Rotation augmentation is disabled because:
# 1. ShapeLLM features are pre-computed without rotation variants
# 2. Captions contain directional references ("vertical", "horizontal")
#    that require orientation-aware features
num_rotations: 1

# ============================================================================
# Training Configuration
# ============================================================================

training:
  # Two-stage training
  num_epochs_stage1: 30   # Grounding establishment
  num_epochs_stage2: 40   # Global alignment with hard negatives

  # Optimization
  batch_size: 64
  learning_rate: 1.0e-4
  weight_decay: 0.05
  warmup_epochs: 3
  min_lr: 1.0e-6
  max_grad_norm: 1.0

  # Stage 2 learning rate reduction
  stage2_lr_factor: 0.5

  # Loss weights
  lambda_global: 1.0        # Global contrastive loss
  lambda_local: 0.5         # Local (per-slot) contrastive loss
  lambda_consist: 0.5       # Grounding consistency loss
  lambda_diverse: 0.2       # Grounding diversity loss
  lambda_conf_reg: 0.1      # Confidence regularization

  # Stage 1 uses reduced global loss weight
  lambda_global_stage1: 0.2

  # Hard negative mining (at stage transition)
  hard_neg_k: 20
  hard_neg_text_threshold: 0.8
  hard_neg_num_seeds: 16
  hard_neg_per_seed: 3

  # Data loading
  num_workers: 4
  pin_memory: true

  # Logging and checkpointing
  log_every: 100
  save_every: 5             # Save checkpoint every N epochs
  save_every_steps: 0       # Save checkpoint every N steps (0 = disabled)
  use_wandb: false

  # Hardware optimization
  gradient_checkpointing: false   # Enable to reduce GPU memory usage
  empty_cache_every_epoch: true   # Clear GPU cache after each epoch

# ============================================================================
# Evaluation Configuration
# ============================================================================

evaluation:
  # Retrieval tasks
  tasks:
    - text_to_brep
    - text_to_pc
    - brep_to_pc
    - pc_to_brep

  # Metrics
  metrics:
    - recall@1
    - recall@5
    - recall@10
    - mrr

  # Batch size for evaluation
  batch_size: 128
